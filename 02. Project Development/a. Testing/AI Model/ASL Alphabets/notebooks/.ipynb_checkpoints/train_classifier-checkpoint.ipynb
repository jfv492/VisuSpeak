{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "from glob import glob\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "\n",
    "from keras.layers import Flatten, Dense, GlobalAveragePooling2D, Dropout, Convolution2D, MaxPooling2D, Activation\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import History, ModelCheckpoint\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import Adadelta, Adagrad, Adam, Adamax, Nadam\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## LOAD BOTTLENECK FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#LOAD DATA\n",
    "\n",
    "vgg_train = np.load(\"../bottleneck_features/vgg16_train_bottleneck_features.pkl_01.npy\")\n",
    "vgg_train_y = np.load(\"../bottleneck_features/vgg16_train_bottleneck_labels.pkl\")\n",
    "vgg_test = np.load(\"../bottleneck_features/vgg16_test_bottleneck_features.pkl_01.npy\")\n",
    "vgg_test_y = np.load(\"../bottleneck_features/vgg16_test_bottleneck_labels.pkl\")\n",
    "\n",
    "res_train = np.load(\"../bottleneck_features/resnet_train_bottleneck_features.pkl_01.npy\")\n",
    "res_train_y = np.load(\"../bottleneck_features/resnet_train_bottleneck_labels.pkl\")\n",
    "res_test = np.load(\"../bottleneck_features/resnet_test_bottleneck_features.pkl_01.npy\")\n",
    "res_test_y = np.load(\"../bottleneck_features/resnet_test_bottleneck_labels.pkl\")\n",
    "\n",
    "inc_train = np.load(\"../bottleneck_features/inception_train_bottleneck_features.pkl_01.npy\")\n",
    "inc_train_y = np.load(\"../bottleneck_features/inception_train_bottleneck_labels.pkl\")\n",
    "inc_test = np.load(\"../bottleneck_features/inception_test_bottleneck_features.pkl_01.npy\")\n",
    "inc_test_y = np.load(\"../bottleneck_features/inception_test_bottleneck_labels.pkl\")\n",
    "\n",
    "xc_train = np.load(\"../bottleneck_features/xception_train_bottleneck_features.pkl_01.npy\")\n",
    "xc_train_y = np.load(\"../bottleneck_features/xception_train_bottleneck_labels.pkl\")\n",
    "xc_test = np.load(\"../bottleneck_features/xception_test_bottleneck_features.pkl_01.npy\")\n",
    "xc_test_y = np.load(\"../bottleneck_features/xception_test_bottleneck_labels.pkl\")\n",
    "\n",
    "mob_train = np.load(\"../bottleneck_features/mobilenet_train_bottleneck_features.pkl_01.npy\")\n",
    "mob_train_y = np.load(\"../bottleneck_features/mobilenet_train_bottleneck_labels.pkl\")\n",
    "mob_test = np.load(\"../bottleneck_features/mobilenet_test_bottleneck_features.pkl_01.npy\")\n",
    "mob_test_y = np.load(\"../bottleneck_features/mobilenet_test_bottleneck_labels.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1504, 7, 7, 512) (374, 7, 7, 512)\n",
      "(1504,) (374,)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(vgg_train).shape, np.array(vgg_test).shape)\n",
    "print(np.array(vgg_train_y).shape, np.array(vgg_test_y).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1504, 1, 1, 2048) (374, 1, 1, 2048)\n",
      "(1504,) (374,)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(res_train).shape, np.array(res_test).shape)\n",
    "print(np.array(res_train_y).shape, np.array(res_test_y).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1504, 6, 6, 2048) (374, 6, 6, 2048)\n",
      "(1504,) (374,)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(inc_train).shape, np.array(inc_test).shape)\n",
    "print(np.array(inc_train_y).shape, np.array(inc_test_y).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1504, 8, 8, 2048) (374, 8, 8, 2048)\n",
      "(1504,) (374,)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(xc_train).shape, np.array(xc_test).shape)\n",
    "print(np.array(xc_train_y).shape, np.array(xc_test_y).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1504, 8, 8, 1024) (374, 8, 8, 1024)\n",
      "(1504,) (374,)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(mob_train).shape, np.array(mob_test).shape)\n",
    "print(np.array(mob_train_y).shape, np.array(mob_test_y).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## TRAIN CLASSIFICATION BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg = [vgg_train, vgg_train_y, vgg_test, vgg_test_y]\n",
    "res = [res_train, res_train_y, res_test, res_test_y]\n",
    "inc = [inc_train, inc_train_y, inc_test, inc_test_y]\n",
    "xc = [xc_train, xc_train_y, xc_test,xc_test_y]\n",
    "mob = [mob_train, mob_train_y, mob_test, mob_test_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, \n",
    "                           min_samples_split=2, min_samples_leaf=1, \n",
    "                           min_weight_fraction_leaf=0.0, max_features='auto', \n",
    "                           max_leaf_nodes=None, min_impurity_split=1e-07, \n",
    "                           bootstrap=True, oob_score=False, n_jobs=-1, \n",
    "                           random_state=123, verbose=0, warm_start=False, \n",
    "                           class_weight=None)\n",
    "\n",
    "scores = []\n",
    "for net in [vgg, res, inc, xc, mob]:\n",
    "    train_y, test_y, train_x, test_x = my_preprocess(net)\n",
    "    rf.fit(X=train_x, y=train_y)\n",
    "    scores.append(rf.score(X=test_x, y=test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(zip([\"vgg\", \"res\", \"inc\", \"xc\", \"mob\"], scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_preprocess(x):\n",
    "    train_y = to_categorical(x[1], num_classes=26)\n",
    "    test_y = to_categorical(x[3], num_classes=26)\n",
    "    reshaped_test = x[2].reshape(x[2].shape[0], -1)\n",
    "    reshaped_train = x[0].reshape(x[0].shape[0], -1)\n",
    "    return (train_y, test_y, reshaped_train, reshaped_test)\n",
    "    assert(train_x.shape[1] == test_x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n",
      "model loaded.\n",
      "fitting model...\n",
      "Train on 1504 samples, validate on 374 samples\n",
      "Epoch 1/5\n",
      "1504/1504 [==============================] - 2s - loss: 8.0657 - acc: 0.4109 - val_loss: 3.4329 - val_acc: 0.7059\n",
      "Epoch 2/5\n",
      "1504/1504 [==============================] - 1s - loss: 2.1014 - acc: 0.8105 - val_loss: 1.1687 - val_acc: 0.8850\n",
      "Epoch 3/5\n",
      "1504/1504 [==============================] - 1s - loss: 0.5225 - acc: 0.9435 - val_loss: 0.5925 - val_acc: 0.9305\n",
      "Epoch 4/5\n",
      "1504/1504 [==============================] - 1s - loss: 0.1895 - acc: 0.9807 - val_loss: 0.0750 - val_acc: 0.9813\n",
      "Epoch 5/5\n",
      "1504/1504 [==============================] - 1s - loss: 0.0262 - acc: 0.9907 - val_loss: 0.0622 - val_acc: 0.9866\n",
      "done!\n",
      "saving logs...\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "batch_size = 32\n",
    "nb_classes = 26\n",
    "nb_epoch = 100\n",
    "image_size = 244\n",
    "num_channels = 'rgb'\n",
    "#################\n",
    "\n",
    "names = [\"vgg\", \"res\", \"inc\", \"xc\", \"mob\"]\n",
    "c = 0\n",
    "\n",
    "for net in [vgg]:\n",
    "    train_y, test_y, train_x, test_x = my_preprocess(net)\n",
    "    \n",
    "    filepath = \"snapshot_\" + names[c] + \"_weights.hdf5\"\n",
    "    \n",
    "    # TO SAVE CONTINUOUS SNAPSHOTS\n",
    "    save_snapshots = ModelCheckpoint(filepath,\n",
    "                                     monitor='val_acc',\n",
    "                                     save_best_only=True,\n",
    "                                     save_weights_only=True,\n",
    "                                     mode='max',\n",
    "                                     verbose=1)\n",
    "    \n",
    "    # Save loss history\n",
    "    class LossHistory(Callback):\n",
    "        def on_train_begin(self, logs={}):\n",
    "            self.losses = []\n",
    "            self.accuracy = []\n",
    "\n",
    "        def on_batch_end(self, batch, logs={}):\n",
    "            self.losses.append(logs.get('loss'))\n",
    "            self.accuracy.append(logs.get('acc'))\n",
    "\n",
    "    loss_history = LossHistory()\n",
    "    callbacks_list = [save_snapshots, loss_history]\n",
    "    \n",
    "    # LOAD MODEL\n",
    "    \n",
    "    print(\"loading model...\")\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(256, input_shape=(train_x.shape[1], )))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(\"model loaded.\")\n",
    "    \n",
    "    # TRAIN MODEL\n",
    "    \n",
    "    print(\"fitting model...\")\n",
    "    \n",
    "    neural_net = model.fit(train_x, train_y, batch_size=128, epochs=5, verbose=1, callbacks=None, \n",
    "                         validation_split=0.0, validation_data=(test_x, test_y), shuffle=True, \n",
    "                         class_weight=None, sample_weight=None, initial_epoch=0)\n",
    "    \n",
    "    print(\"done!\")\n",
    "    print(\"saving logs + weights...\")\n",
    "    \n",
    "    # SAVE WEIGHTS + LOGS\n",
    "    #model.save_weights(filepath)\n",
    "    \n",
    "    evaluation_cost = neural_net.history['val_loss']\n",
    "    evaluation_accuracy = neural_net.history['val_acc']\n",
    "    training_cost = neural_net.history['loss']\n",
    "    training_accuracy = neural_net.history['acc']\n",
    "\n",
    "    np.save(names[c] + \"_evaluation_cost.npy\", evaluation_cost)\n",
    "    np.save(names[c] + \"_evaluation_accuracy.npy\", evaluation_accuracy)\n",
    "    np.save(names[c] + \"_training_cost.npy\", training_cost)\n",
    "    np.save(names[c] + \"_training_accuracy.npy\", training_accuracy)\n",
    "    c+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.35837766084265199, 0.71143617021276595],\n",
       " 'loss': [9.3892843266750905, 3.6352081095918698]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_net.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "\n",
    "# Model Checkpoint\n",
    "filepath = \"back_up_\" + \"_weights.hdf5\"\n",
    "\n",
    "save_snapshots = ModelCheckpoint(filepath,\n",
    "                                 monitor='val_acc',\n",
    "                                 save_best_only=True,\n",
    "                                 save_weights_only=True,\n",
    "                                 mode='max',\n",
    "                                 verbose=1)\n",
    "\n",
    "\n",
    "# Save loss history\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.accuracy = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.accuracy.append(logs.get('acc'))\n",
    "\n",
    "loss_history = LossHistory()\n",
    "#callbacks_list = [loss_history]\n",
    "\n",
    "# define train data generator\n",
    "train_datagen = ImageDataGenerator(rescale=1.,\n",
    "                                   featurewise_center=True,\n",
    "                                   rotation_range=15.0,\n",
    "                                   width_shift_range=0.15,\n",
    "                                   height_shift_range=0.15)\n",
    "\n",
    "train_datagen.mean = np.array([103.939, 116.779, 123.68], dtype=np.float32).reshape(1, 1, 3)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "            args['train'],\n",
    "            target_size=(244, 244),\n",
    "            batch_size=batch_size,\n",
    "            class_mode=\"categorical\",\n",
    "            color_mode=colour,\n",
    "            shuffle=False\n",
    "            )\n",
    "\n",
    "# define validation data generator\n",
    "test_datagen = ImageDataGenerator(rescale=1.,\n",
    "                                  featurewise_center=True)\n",
    "\n",
    "test_datagen.mean = np.array([103.939, 116.779, 123.68], dtype=np.float32).reshape(1, 1, 3)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "            args['test'],\n",
    "            target_size=(244, 244),\n",
    "            batch_size=batch_size,\n",
    "            class_mode=\"categorical\",\n",
    "            color_mode=colour,\n",
    "            shuffle=False\n",
    "            )\n",
    "\n",
    "##############################\n",
    "steps_per_epoch = int(train_generator.samples//batch_size)\n",
    "validation_steps = int(test_generator.samples//batch_size)\n",
    "##############################\n",
    "\n",
    "# train model\n",
    "my_history = model.fit_generator(\n",
    "    generator=train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=nb_epoch,\n",
    "    verbose=1,\n",
    "    #callbacks=callbacks_list,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    class_weight=None,\n",
    "    pickle_safe=False)\n",
    "\n",
    "#save_loss_history = loss_history.losses\n",
    "#save_accuracy_history = loss_history.accuracy\n",
    "#np.savetxt(\"loss_history.txt\", save_loss_history, delimiter=\",\")\n",
    "#np.savetxt(\"accuracy_history.txt\", save_accuracy_history, delimiter=\",\")\n",
    "#my_model.save_weights('my_model_weights.h5')\n",
    "\n",
    "evaluation_cost = my_history.history['val_loss']\n",
    "evaluation_accuracy = my_history.history['val_acc']\n",
    "training_cost = my_history.history['loss']\n",
    "training_accuracy = my_history.history['acc']\n",
    "\n",
    "np.save(\"evaluation_cost.npy\", evaluation_cost)\n",
    "np.save(\"evaluation_accuracy.npy\", evaluation_accuracy)\n",
    "np.save(\"training_cost.npy\", training_cost)\n",
    "np.save(\"training_accuracy.npy\", training_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#vgg_net_1\n",
    "\n",
    "evaluation_cost = np.load(\"../bottleneck_features/transfer_learning/bu_vgg_1/evaluation_cost.npy\")\n",
    "evaluation_accuracy = np.load(\"../bottleneck_features/transfer_learning/bu_vgg_1/evaluation_accuracy.npy\")\n",
    "training_cost = np.load(\"../bottleneck_features/transfer_learning/bu_vgg_1/training_cost.npy\")\n",
    "training_accuracy = np.load(\"../bottleneck_features/transfer_learning/bu_vgg_1/training_accuracy.npy\")\n",
    "\n",
    "my_plot = plot_logs(evaluation_cost, evaluation_accuracy, training_cost, training_accuracy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOTTLENECK + NON-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create label dictionary\n",
    "label_dict = {}\n",
    "for pos, letter in enumerate(string.ascii_lowercase):\n",
    "    label_dict[pos] = letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from seaborn import heatmap\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_y = to_categorical(res_train_y, num_classes=26)\n",
    "test_y = to_categorical(res_test_y, num_classes=26)\n",
    "\n",
    "train_y.shape, test_y.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reshaped_test = res_test.reshape(res_test.shape[0], -1)\n",
    "reshaped_train = res_train.reshape(res_train.shape[0], -1)\n",
    "\n",
    "reshaped_train.shape, reshaped_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ovr = OneVsRestClassifier(LinearSVC(random_state=123), n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ovr.fit(X=reshaped_train, y=train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ovr.score(X=reshaped_test, y=test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf.fit(X=reshaped_train, y=train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf.score(X=reshaped_test, y=test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(base_estimator=None, n_estimators=50, \n",
    "                         learning_rate=1.0, algorithm='SAMME.R', random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ada.fit(X=reshaped_train, y=res_train_y[:res_train.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ada.score(X=reshaped_test, y=(res_test_y[:res_test.shape[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gd = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, \n",
    "                                subsample=1.0, criterion='friedman_mse', min_samples_split=2, \n",
    "                                min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                                max_depth=3, min_impurity_split=1e-07, init=None, \n",
    "                                random_state=123, max_features=None, verbose=0, \n",
    "                                max_leaf_nodes=None, warm_start=False, presort='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gd.fit(X=reshaped_train, y=res_train_y[:res_train.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gd.score(X=reshaped_test, y=(res_test_y[:res_test.shape[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from processing import preprocess_for_vgg\n",
    "import cv2\n",
    "\n",
    "vgg_model = VGG16(include_top=False)\n",
    "res_model = ResNet50(include_top=False)import matplotlib \n",
    "\n",
    "\n",
    "matplotlib.use('Agg') \n",
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_logs(history):\n",
    "    evaluation_cost = history.history['val_loss']\n",
    "    evaluation_accuracy = history.history['val_acc']\n",
    "    training_cost = history.history['loss']\n",
    "    training_accuracy = history.history['acc']\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    f.set_figwidth(10)\n",
    "    ax1.plot(evaluation_cost,label= 'test')\n",
    "    ax1.plot(training_cost, label='train')\n",
    "    ax1.set_title('Cost')\n",
    "    ax1.legend()\n",
    "    ax2.plot(evaluation_accuracy, label='test')\n",
    "    ax2.plot(training_accuracy, label='train')\n",
    "    ax2.set_title('Accuracy')\n",
    "    ax2.legend(loc='lower right')\n",
    "\n",
    "def preprocess_for_vgg(image, size=224, color=True):\n",
    "    image = cv2.resize(image, (size, size))\n",
    "    x = np.array(image, dtype=float)\n",
    "    x_fake_batch = x.reshape(1, *x.shape)\n",
    "    x = x_fake_batch\n",
    "    # 'BGR' -> 'RGB'\n",
    "    #x = x[:, :, :, ::-1]\n",
    "    if color:\n",
    "        # Zero-center by mean pixel\n",
    "        x[:, :, :, 0] -= 103.939\n",
    "        x[:, :, :, 1] -= 116.779\n",
    "        x[:, :, :, 2] -= 123.68\n",
    "    return x\n",
    "\n",
    "#convert class letters to integer values\n",
    "\n",
    "img_a = cv2.imread(\"../../train_data/FINAL_DATA/grayscale/train/a/my_gray_processed_a_029.png\",0)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 and OpenCV3",
   "language": "python",
   "name": "py3cv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
