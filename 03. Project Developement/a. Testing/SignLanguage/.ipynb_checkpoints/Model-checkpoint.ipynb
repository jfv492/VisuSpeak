{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0ae35fe",
   "metadata": {},
   "source": [
    "### dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50809698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "class SignLanguageMNIST(Dataset):\n",
    "    \"\"\"Sign Language classification dataset.\n",
    "\n",
    "    Utility for loading Sign Language dataset into PyTorch. Dataset posted on\n",
    "    Kaggle in 2017, by an unnamed author with username `tecperson`:\n",
    "    https://www.kaggle.com/datamunge/sign-language-mnist\n",
    "\n",
    "    Each sample is 1 x 1 x 28 x 28, and each label is a scalar.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_label_mapping():\n",
    "        \"\"\"\n",
    "        We map all labels to [0, 23]. This mapping from dataset labels [0, 23]\n",
    "        to letter indices [0, 25] is returned below.\n",
    "        \"\"\"\n",
    "        mapping = list(range(25))\n",
    "        mapping.pop(9)\n",
    "        return mapping\n",
    "\n",
    "    @staticmethod\n",
    "    def read_label_samples_from_csv(path: str):\n",
    "        \"\"\"\n",
    "        Assumes first column in CSV is the label and subsequent 28^2 values\n",
    "        are image pixel values 0-255.\n",
    "        \"\"\"\n",
    "        mapping = SignLanguageMNIST.get_label_mapping()\n",
    "        labels, samples = [], []\n",
    "        with open(path) as f:\n",
    "            _ = next(f)  # skip header\n",
    "            for line in csv.reader(f):\n",
    "                label = int(line[0])\n",
    "                labels.append(mapping.index(label))\n",
    "                samples.append(list(map(int, line[1:])))\n",
    "        return labels, samples\n",
    "\n",
    "    def __init__(self,\n",
    "            path: str=\"data/sign_mnist_train.csv\",\n",
    "            mean: List[float]=[0.485],\n",
    "            std: List[float]=[0.229]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path: Path to `.csv` file containing `label`, `pixel0`, `pixel1`...\n",
    "        \"\"\"\n",
    "        labels, samples = SignLanguageMNIST.read_label_samples_from_csv(path)\n",
    "        self._samples = np.array(samples, dtype=np.uint8).reshape((-1, 28, 28, 1))\n",
    "        self._labels = np.array(labels, dtype=np.uint8).reshape((-1, 1))\n",
    "\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomResizedCrop(28, scale=(0.8, 1.2)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=self._mean, std=self._std)])\n",
    "\n",
    "        return {\n",
    "            'image': transform(self._samples[idx]).float(),\n",
    "            'label': torch.from_numpy(self._labels[idx]).float()\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abda4859",
   "metadata": {},
   "source": [
    "verify our dataset utility functions by loading the SignLanguageMNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2ed62f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_loaders(batch_size=32):\n",
    "    trainset = SignLanguageMNIST('data/sign_mnist_train.csv')\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    testset = SignLanguageMNIST('data/sign_mnist_test.csv')\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b310cf",
   "metadata": {},
   "source": [
    "Create a sample dataset loader using DataLoader and print the first element of that loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b596d1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': tensor([[[[-1.7240, -1.2445, -0.8507,  ...,  0.6906,  0.7077,  0.7248],\n",
      "          [-1.6555, -1.1760, -0.7993,  ...,  0.7077,  0.7077,  0.7248],\n",
      "          [-1.5699, -1.1247, -0.7650,  ...,  0.7077,  0.7248,  0.7248],\n",
      "          ...,\n",
      "          [-0.6109, -0.4739, -0.3369,  ...,  1.0673,  1.0844,  1.0673],\n",
      "          [-0.6623, -0.5253, -0.3712,  ...,  1.0331,  1.0502,  1.0673],\n",
      "          [-0.6965, -0.5596, -0.3883,  ...,  0.9988,  1.0159,  1.0331]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7933,  0.8104,  0.8104,  ...,  0.7419,  0.2624, -0.8849],\n",
      "          [ 0.8104,  0.8104,  0.8104,  ...,  0.7077,  0.5707, -0.4911],\n",
      "          [ 0.8276,  0.8276,  0.8276,  ...,  0.7248,  0.7591,  0.1254],\n",
      "          ...,\n",
      "          [ 1.0331,  1.0502,  1.0502,  ...,  0.9988,  0.9988,  0.9646],\n",
      "          [ 1.0331,  1.0502,  1.0502,  ...,  0.9988,  0.9988,  0.9817],\n",
      "          [ 1.0502,  1.0502,  1.0502,  ...,  0.9988,  0.9817,  0.9817]]]]), 'label': tensor([[ 3.],\n",
      "        [17.]])}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    loader, _ = get_train_test_loaders(2)\n",
    "    print(next(iter(loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17d1cc2",
   "metadata": {},
   "source": [
    "### train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2527ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0,     0] loss: 3.205600\n",
      "[0,   100] loss: 3.211615\n",
      "[0,   200] loss: 3.197676\n",
      "[0,   300] loss: 3.023794\n",
      "[0,   400] loss: 2.781263\n",
      "[0,   500] loss: 2.505607\n",
      "[0,   600] loss: 2.261646\n",
      "[0,   700] loss: 2.057641\n",
      "[0,   800] loss: 1.878600\n",
      "[1,     0] loss: 0.441917\n",
      "[1,   100] loss: 0.495896\n",
      "[1,   200] loss: 0.433333\n",
      "[1,   300] loss: 0.400881\n",
      "[1,   400] loss: 0.376007\n",
      "[1,   500] loss: 0.363435\n",
      "[1,   600] loss: 0.342630\n",
      "[1,   700] loss: 0.324584\n",
      "[1,   800] loss: 0.314123\n",
      "[2,     0] loss: 0.358849\n",
      "[2,   100] loss: 0.186154\n",
      "[2,   200] loss: 0.161061\n",
      "[2,   300] loss: 0.157594\n",
      "[2,   400] loss: 0.149835\n",
      "[2,   500] loss: 0.152977\n",
      "[2,   600] loss: 0.149730\n",
      "[2,   700] loss: 0.143873\n",
      "[2,   800] loss: 0.146049\n",
      "[3,     0] loss: 0.183543\n",
      "[3,   100] loss: 0.104116\n",
      "[3,   200] loss: 0.119149\n",
      "[3,   300] loss: 0.100280\n",
      "[3,   400] loss: 0.098831\n",
      "[3,   500] loss: 0.091762\n",
      "[3,   600] loss: 0.095163\n",
      "[3,   700] loss: 0.094219\n",
      "[3,   800] loss: 0.092902\n",
      "[4,     0] loss: 0.050471\n",
      "[4,   100] loss: 0.087845\n",
      "[4,   200] loss: 0.092633\n",
      "[4,   300] loss: 0.084839\n",
      "[4,   400] loss: 0.081774\n",
      "[4,   500] loss: 0.078158\n",
      "[4,   600] loss: 0.075077\n",
      "[4,   700] loss: 0.074458\n",
      "[4,   800] loss: 0.073334\n",
      "[5,     0] loss: 0.397869\n",
      "[5,   100] loss: 0.051456\n",
      "[5,   200] loss: 0.060655\n",
      "[5,   300] loss: 0.060377\n",
      "[5,   400] loss: 0.059097\n",
      "[5,   500] loss: 0.059508\n",
      "[5,   600] loss: 0.059078\n",
      "[5,   700] loss: 0.057225\n",
      "[5,   800] loss: 0.058241\n",
      "[6,     0] loss: 0.003005\n",
      "[6,   100] loss: 0.015877\n",
      "[6,   200] loss: 0.034153\n",
      "[6,   300] loss: 0.044792\n",
      "[6,   400] loss: 0.042935\n",
      "[6,   500] loss: 0.041418\n",
      "[6,   600] loss: 0.044899\n",
      "[6,   700] loss: 0.045520\n",
      "[6,   800] loss: 0.046674\n",
      "[7,     0] loss: 0.019114\n",
      "[7,   100] loss: 0.038884\n",
      "[7,   200] loss: 0.046386\n",
      "[7,   300] loss: 0.044854\n",
      "[7,   400] loss: 0.042225\n",
      "[7,   500] loss: 0.042440\n",
      "[7,   600] loss: 0.042389\n",
      "[7,   700] loss: 0.042616\n",
      "[7,   800] loss: 0.041566\n",
      "[8,     0] loss: 0.020522\n",
      "[8,   100] loss: 0.014529\n",
      "[8,   200] loss: 0.025224\n",
      "[8,   300] loss: 0.033223\n",
      "[8,   400] loss: 0.033254\n",
      "[8,   500] loss: 0.033789\n",
      "[8,   600] loss: 0.033905\n",
      "[8,   700] loss: 0.034819\n",
      "[8,   800] loss: 0.036101\n",
      "[9,     0] loss: 0.020823\n",
      "[9,   100] loss: 0.053212\n",
      "[9,   200] loss: 0.047423\n",
      "[9,   300] loss: 0.046247\n",
      "[9,   400] loss: 0.049263\n",
      "[9,   500] loss: 0.044992\n",
      "[9,   600] loss: 0.043158\n",
      "[9,   700] loss: 0.041696\n",
      "[9,   800] loss: 0.041606\n",
      "[10,     0] loss: 0.049151\n",
      "[10,   100] loss: 0.017096\n",
      "[10,   200] loss: 0.015091\n",
      "[10,   300] loss: 0.013096\n",
      "[10,   400] loss: 0.012740\n",
      "[10,   500] loss: 0.012497\n",
      "[10,   600] loss: 0.012351\n",
      "[10,   700] loss: 0.012203\n",
      "[10,   800] loss: 0.011708\n",
      "[11,     0] loss: 0.001936\n",
      "[11,   100] loss: 0.012001\n",
      "[11,   200] loss: 0.011742\n",
      "[11,   300] loss: 0.011221\n",
      "[11,   400] loss: 0.010621\n",
      "[11,   500] loss: 0.010549\n",
      "[11,   600] loss: 0.010422\n",
      "[11,   700] loss: 0.009791\n",
      "[11,   800] loss: 0.009548\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "# from step_2_dataset import get_train_test_loaders\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 6, 3)\n",
    "        self.conv3 = nn.Conv2d(6, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 48)\n",
    "        self.fc3 = nn.Linear(48, 25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def main():\n",
    "    net = Net().float()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    trainloader, _ = get_train_test_loaders()\n",
    "    for epoch in range(12):  # loop over the dataset multiple times\n",
    "        train(net, criterion, optimizer, trainloader, epoch)\n",
    "        scheduler.step()\n",
    "    torch.save(net.state_dict(), \"checkpoint.pth\")\n",
    "\n",
    "\n",
    "def train(net, criterion, optimizer, trainloader, epoch):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs = Variable(data['image'].float())\n",
    "        labels = Variable(data['label'].long())\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels[:, 0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print('[%d, %5d] loss: %.6f' % (epoch, i, running_loss / (i + 1)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2820b455",
   "metadata": {},
   "source": [
    "### evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52b6ea90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== PyTorch ==========\n",
      "Training accuracy: 99.8\n",
      "Validation accuracy: 97.6\n",
      "========== ONNX ==========\n",
      "Training accuracy: 99.8\n",
      "Validation accuracy: 97.7\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "#from step_2_dataset import get_train_test_loaders\n",
    "#from step_3_train import Net\n",
    "\n",
    "\n",
    "def evaluate(outputs: Variable, labels: Variable) -> float:\n",
    "    \"\"\"Evaluate neural network outputs against non-one-hotted labels.\"\"\"\n",
    "    Y = labels.numpy()\n",
    "    Yhat = np.argmax(outputs, axis=1)\n",
    "    return float(np.sum(Yhat == Y))\n",
    "\n",
    "\n",
    "def batch_evaluate(\n",
    "        net: Net,\n",
    "        dataloader: torch.utils.data.DataLoader) -> float:\n",
    "    \"\"\"Evaluate neural network in batches, if dataset is too large.\"\"\"\n",
    "    score = n = 0.0\n",
    "    for batch in dataloader:\n",
    "        n += len(batch['image'])\n",
    "        outputs = net(batch['image'])\n",
    "        if isinstance(outputs, torch.Tensor):\n",
    "            outputs = outputs.detach().numpy()\n",
    "        score += evaluate(outputs, batch['label'][:, 0])\n",
    "    return score / n\n",
    "\n",
    "\n",
    "def validate():\n",
    "    trainloader, testloader = get_train_test_loaders()\n",
    "    net = Net().float().eval()\n",
    "\n",
    "    pretrained_model = torch.load(\"checkpoint.pth\")\n",
    "    net.load_state_dict(pretrained_model)\n",
    "\n",
    "    print('=' * 10, 'PyTorch', '=' * 10)\n",
    "    train_acc = batch_evaluate(net, trainloader) * 100.\n",
    "    print('Training accuracy: %.1f' % train_acc)\n",
    "    test_acc = batch_evaluate(net, testloader) * 100.\n",
    "    print('Validation accuracy: %.1f' % test_acc)\n",
    "\n",
    "    trainloader, testloader = get_train_test_loaders(1)\n",
    "\n",
    "    # export to onnx\n",
    "    fname = \"signlanguage.onnx\"\n",
    "    dummy = torch.randn(1, 1, 28, 28)\n",
    "    torch.onnx.export(net, dummy, fname, input_names=['input'])\n",
    "\n",
    "    # check exported model\n",
    "    model = onnx.load(fname)\n",
    "    onnx.checker.check_model(model)  # check model is well-formed\n",
    "\n",
    "    # create runnable session with exported model\n",
    "    ort_session = ort.InferenceSession(fname)\n",
    "    net = lambda inp: ort_session.run(None, {'input': inp.data.numpy()})[0]\n",
    "\n",
    "    print('=' * 10, 'ONNX', '=' * 10)\n",
    "    train_acc = batch_evaluate(net, trainloader) * 100.\n",
    "    print('Training accuracy: %.1f' % train_acc)\n",
    "    test_acc = batch_evaluate(net, testloader) * 100.\n",
    "    print('Validation accuracy: %.1f' % test_acc)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab06890d",
   "metadata": {},
   "source": [
    "### camera.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8672179e",
   "metadata": {},
   "source": [
    "### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "972175fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@269.484] global /private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_5a1v4y7k9y/croot/opencv-suite_1676472757237/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 30\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# preprocess data\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[43mcenter_crop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2GRAY)\n\u001b[1;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(frame, (\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m))\n",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m, in \u001b[0;36mcenter_crop\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcenter_crop\u001b[39m(frame):\n\u001b[0;32m----> 7\u001b[0m     h, w, _ \u001b[38;5;241m=\u001b[39m \u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[1;32m      8\u001b[0m     start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(h \u001b[38;5;241m-\u001b[39m w) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m h \u001b[38;5;241m>\u001b[39m w:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "\n",
    "def center_crop(frame):\n",
    "    h, w, _ = frame.shape\n",
    "    start = abs(h - w) // 2\n",
    "    if h > w:\n",
    "        return frame[start: start + w]\n",
    "    return frame[:, start: start + h]\n",
    "\n",
    "\n",
    "def main():\n",
    "    # constants\n",
    "    index_to_letter = list('ABCDEFGHIKLMNOPQRSTUVWXY')\n",
    "    mean = 0.485 * 255.\n",
    "    std = 0.229 * 255.\n",
    "\n",
    "    # create runnable session with exported model\n",
    "    ort_session = ort.InferenceSession(\"signlanguage.onnx\")\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "\n",
    "        # preprocess data\n",
    "        frame = center_crop(frame)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        x = cv2.resize(frame, (28, 28))\n",
    "        x = (x - mean) / std\n",
    "\n",
    "        x = x.reshape(1, 1, 28, 28).astype(np.float32)\n",
    "        y = ort_session.run(None, {'input': x})[0]\n",
    "\n",
    "        index = np.argmax(y, axis=1)\n",
    "        letter = index_to_letter[int(index)]\n",
    "\n",
    "        cv2.putText(frame, letter, (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 2.0, (0, 255, 0), thickness=2)\n",
    "        cv2.imshow(\"Sign Language Translator\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a57d6f",
   "metadata": {},
   "source": [
    "### Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de83c0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@570.127] global /private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_5a1v4y7k9y/croot/opencv-suite_1676472757237/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "def center_crop(frame):\n",
    "    h, w, _ = frame.shape\n",
    "    start = abs(h - w) // 2\n",
    "    if h > w:\n",
    "        return frame[start: start + w]\n",
    "    return frame[:, start: start + h]\n",
    "\n",
    "def main():\n",
    "    # Constants\n",
    "    index_to_letter = list('ABCDEFGHIKLMNOPQRSTUVWXY')\n",
    "    mean = 0.485 * 255.0\n",
    "    std = 0.229 * 255.0\n",
    "\n",
    "    # Create runnable session with the exported model\n",
    "    ort_session = ort.InferenceSession(\"signlanguage.onnx\")\n",
    "\n",
    "    # Open the camera (default camera)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Camera not available. Exiting...\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"Failed to capture a frame from the camera.\")\n",
    "            break\n",
    "\n",
    "        # Preprocess data\n",
    "        frame = center_crop(frame)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        x = cv2.resize(frame, (28, 28))\n",
    "        x = (x - mean) / std\n",
    "\n",
    "        x = x.reshape(1, 1, 28, 28).astype(np.float32)\n",
    "        y = ort_session.run(None, {'input': x})[0]\n",
    "\n",
    "        index = np.argmax(y, axis=1)\n",
    "        letter = index_to_letter[int(index)]\n",
    "\n",
    "        cv2.putText(frame, letter, (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 2.0, (0, 255, 0), thickness=2)\n",
    "        cv2.imshow(\"Sign Language Translator\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
